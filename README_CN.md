<p align="left">
    ä¸­æ–‡ | <a href="README.md">English</a>
</p>

<p align="center">
    <img src="data/chat.jpg" alt="Tiny Qwen äº¤äº’å¼å¯¹è¯">
</p>

# âœ¨ Tiny Qwen

ä¸€ä¸ªç®€æ´æ˜“è¯»çš„ PyTorch ä»£ç åº“ï¼Œç”¨äºé‡å†™ `Qwen3-VL`ï¼ŒåŒæ—¶æ”¯æŒæ–‡æœ¬ä¸è§†è§‰æ¨¡æ€ï¼Œå¹¶å…¼å®¹ç¨ å¯†å’Œæ··åˆä¸“å®¶æ¶æ„ã€‚

å¦‚æœä½ è§‰å¾— Hugging Face çš„ä»£ç å†—é•¿ä¸”éš¾ä»¥é˜…è¯»ï¼Œè¿™ä¸ªä»“åº“æ­£é€‚åˆä½ ï¼

è‹¥éœ€ `Qwen3`ï¼ˆçº¯æ–‡æœ¬ï¼‰ä¸ `Qwen2.5 VL` æ”¯æŒï¼Œè¯·æŸ¥çœ‹[è¿™ä¸ª branch](https://github.com/Emericen/tiny-qwen/tree/legacy/qwen2_5)ã€‚

è‹¥éœ€ `DeepSeek R1`ï¼Œè¯·æŸ¥çœ‹[è¿™ä¸ªä»“åº“](https://github.com/Emericen/tiny-deepseek-r1)ã€‚

æ¬¢è¿å¤§å®¶åŠ æˆ‘çš„ [Discord ](https://discord.gg/sBNnqP9gaY)ç»§ç»­è®¨è®ºï¼

## ğŸ¦‹ å¿«é€Ÿå¼€å§‹

æ¨èä½¿ç”¨ `uv` åˆ›å»ºå¹¶éš”ç¦»è™šæ‹Ÿç¯å¢ƒï¼š

```bash
pip install uv && uv venv

# æ¿€æ´»ç¯å¢ƒ
source .venv/bin/activate # Linux/macOS
.venv\Scripts\activate # Windows

# å®‰è£…ä¾èµ–
uv pip install -r requirements.txt
```

å¯åŠ¨äº¤äº’å¼å¯¹è¯ï¼š

```bash
python run.py
```

**æ³¨æ„ï¼š** `Qwen3` ä»…æ”¯æŒæ–‡æœ¬ã€‚è‹¥è¦åœ¨ `Qwen2.5-VL` ä¸­å¼•ç”¨å›¾ç‰‡ï¼Œè¯·ä½¿ç”¨ `@path/to/image.jpg`ã€‚

```
USER: @data/test-img-1.jpg å‘Šè¯‰æˆ‘è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ
âœ“ Found image: data/test-img-1.jpg
ASSISTANT: è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†å……æ»¡æ´»åŠ›çš„å‘æ—¥è‘µç”°...
```

## ğŸ“ ä»£ç ç¤ºä¾‹

ä½¿ç”¨ `Qwen3VL` ç±»ï¼š

```python
from PIL import Image
from huggingface_hub import snapshot_download
from model.model import Qwen3VL
from model.processor import Processor

image = Image.open("test/data/test-img-1.jpg")

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"},
        ],
    },
]

model_name = "Qwen/Qwen3-VL-4B-Instruct"
weights = snapshot_download(repo_id=model_name, cache_dir=".cache")
model = Qwen3VL.from_pretrained(weights_path=weights, device_map="auto")
processor = Processor.from_pretrained(model_name)

device = next(model.parameters()).device
inputs = processor(messages, add_generation_prompt=True, device=device)

output_ids = model.generate(**inputs, max_new_tokens=64)
print(processor.tokenizer.decode(output_ids[0].tolist()))

print("æµå¼è¾“å‡º:", end=" ", flush=True)
for token_id in model.generate_stream(**inputs, max_new_tokens=64):
    print(processor.tokenizer.decode([token_id]), end="", flush=True)
print()
```
